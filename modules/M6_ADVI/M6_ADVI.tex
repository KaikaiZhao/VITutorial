\documentclass[14pt,dvipsnames]{beamer}

\usetheme{Montpellier}
\usecolortheme{beaver}

\usepackage{amsmath, amssymb, ../../vimacros, hyperref, tikz}
\usepackage{physics}
\usetikzlibrary{positioning, fit, bayesnet, shapes.misc, patterns}
\usepackage[round]{natbib}
\usepackage{mathalfa}
\usepackage{cancel}

\beamertemplatenavigationsymbolsempty

\hypersetup{breaklinks=true, colorlinks=true, linkcolor=blue, urlcolor=blue, citecolor=blue}

\newcommand{\balert}[1]{\textcolor{blue}{#1}}
\newcommand{\galert}[1]{\textcolor{PineGreen}{#1}}

\pgfdeclarepatternformonly{stripes}
{\pgfpointorigin}{\pgfpoint{.4cm}{.4cm}}
{\pgfpoint{.4cm}{.4cm}}
{
	\pgfpathmoveto{\pgfpoint{0cm}{0cm}}
	\pgfpathlineto{\pgfpoint{.4cm}{.4cm}}
	\pgfpathlineto{\pgfpoint{.4cm}{.2cm}}
	\pgfpathlineto{\pgfpoint{.2cm}{0cm}}
	\pgfpathclose
	\pgfusepath{fill}
	\pgfpathmoveto{\pgfpoint{0cm}{0.2cm}}
	\pgfpathlineto{\pgfpoint{0cm}{.4cm}}
	\pgfpathlineto{\pgfpoint{0.2cm}{.4cm}}
	\pgfpathclose
	\pgfusepath{fill}
}

\title{Automatic Differentiation Variational Inference}
\author{Philip Schulz and Wilker Aziz\\
\url{https://github.com/philschulz/VITutorial}}
\date{}

\setbeamertemplate{footline}[frame number]

\begin{document}

\begin{frame}
\maketitle
\end{frame}

\begin{frame}{What we know so far}
    \begin{itemize}
        \item DGMs: \pause probabilistic models parameterised by neural networks \pause
        \item Objective: \pause lowerbound on likelihood (ELBO) \pause
        \begin{itemize}
        		\item \alert{cannot be computed exactly} \\ \pause
        		\textcolor{blue}{we resort to Monte Carlo estimation} \pause		
	\end{itemize}
	\item \alert{But the MC estimator is not differentiable} \pause		        
	\begin{itemize}
       		\item Score function estimator: applicable to any model  \pause
		\item Reparameterised gradients\\
		so far seems applicable only to Gaussian variables
        \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}{Reparameterised gradients: Gaussian}
	We have  seen one case, namely,\\
	~ if $\epsilon \sim \mathcal N(0, I)$ and $Z \sim \mathcal N(\mu,\sigma^2)$\pause\\
	Then
	\begin{equation*}
	\begin{aligned}
		%\epsilon &\sim \mathcal N(0, 1) \\	\pause
		%Z &\sim \mathcal N(\mu, \sigma^2) \\ \pause
		Z &\sim \mu +  \sigma  \epsilon
	\end{aligned}
	\end{equation*}
	and
	\begin{equation*}
	\begin{aligned}
		&\pdv{\lambda} \E[\mathcal N(z|\mu, \sigma^2)]{ g(z) }\\ \pause
		&= \E[\mathcal N(0, I)]{\pdv{\lambda} g(z = \mu + \sigma  \epsilon)} \\ \pause
		&= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon) \pdv{z}{\lambda}}
	\end{aligned}
	\end{equation*}
\end{frame}

\begin{frame}{Reparameterised gradients: Gaussian}
	Location
	\begin{equation*}
	\begin{aligned}
		\pdv{\mu} \E[\mathcal N(z|\mu, \sigma^2)]{ g(z) }
			&= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon) \pdv{z}{\mu}} \\ \pause
		&= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon)} \pause
	\end{aligned}
	\end{equation*}
	
	Scale
	\begin{equation*}
	\begin{aligned}
		\pdv{\sigma} \E[\mathcal N(z|\mu, \sigma^2)]{ g(z) } &= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon) \pdv{z}{\sigma}} \\ \pause
		&= \E[\mathcal N(0, I)]{\pdv{z} g(z = \mu + \sigma  \epsilon)  \epsilon} \pause
	\end{aligned}
	\end{equation*}
	
\end{frame}

\section{Multivariate calculus recap}

\begin{frame}{Multivariate calculus recap}

Let $x \in \mathbb R^K$ and let $\mathcal T: \mathbb R^K \to \mathbb R^K$ be differentiable and invertible
\begin{itemize}
	\item $y = \mathcal T(x)$
	\item $x = \inv{\mathcal T}(y)$
\end{itemize}

\end{frame}

\begin{frame}{Jacobian}

	The Jacobian matrix $\mathbf J = \jac{\mathcal T}{x} $ of  $\mathcal T$ \\
	~assessed at $x$ is the matrix of partial derivatives
	\begin{equation*}
		J_{ij} = \pdv{y_i}{x_j} 
	\end{equation*} 
	
	\pause
	Inverse function theorem
	\begin{equation*}
		\jac{\inv{\mathcal T}}{y} = \left( \jac{\mathcal T}{x} \right)^{-1}
	\end{equation*}
	
\end{frame}

\begin{frame}{Differential (or inifinitesimal)}

	The {\bf differential} $\dd x$ of $x$ \\
	~ refers to an \emph{infinitely small} change in $x$\\ \pause
	\vspace{10pt}

	We can relate the differential $\dd y$ of $y = \mathcal T(x)$ to $\dd x$ \pause
	\begin{itemize}
		\item Scalar case
		\begin{equation*}
			\dd y = \alert{\mathcal T'(x)} \dd x = \alert{\dv{y}{x}} \dd x = \alert{\dv{x}T(x)} \dd x
		\end{equation*}
		where \alert{$\dv*{y}{x}$} is the \emph{derivative} of $y$ wrt $x$ \pause
		\item Multivariate case
		\begin{equation*}
        			\begin{aligned}
			        \dd y = \alert{\djac{\mathcal T}{x}} \dd x % = \alert{\abs{\pdv{x}T(x)}} \dd x % in some texts people will find this notation
		        	\end{aligned}
	        	\end{equation*}
		the absolute value absorbs the orientation 
	\end{itemize}

\end{frame}

\begin{frame}{Integration by substitution}	
	We can integrate a function $g(x)$ \\
	~ by substituting $x = \inv{ \mathcal T}(y)$
	\begin{equation*}
	\begin{aligned}
		\int g(\balert{x}) \alert{\dd x} \pause &= \int g(\underbrace{\balert{\inv{\mathcal T}(y)}}_{x}) \underbrace{\alert{\djac{\inv{\mathcal T}}{y} \dd y}}_{\dd x} \\ \pause
	\end{aligned}
	\end{equation*}
	
	and similarly for a function $h(y)$
	\begin{equation*}
	\begin{aligned}
		\int h(\balert{y}) \alert{\dd y} \pause &= \int h(\balert{\mathcal T(x)}) \alert{\djac{\mathcal T}{x} \dd x}
	\end{aligned}
	\end{equation*} 

\end{frame}

\begin{frame}{Change of density}

Let $X$ take on values in $\mathbb R^K$ with density $f_X(x)$\\ \pause
~ and recall that $y = \mathcal T(x)$ and $x = \inv{\mathcal T}(y)$\\ \pause

~

Then $\mathcal T$ induces a density $f_Y(y)$ expressed as
\begin{equation*}
f_Y(y) = f_X(x = \inv{\mathcal T}(y)) \djac{\inv{\mathcal T}}{y}
\end{equation*} \pause
and then it follows that
\begin{equation*}
f_X(x) = f_Y(y=\mathcal T(x)) \djac{\mathcal T}{x}
\end{equation*}

	
\end{frame}

\section{Reparameterised gradients revisited}

\begin{frame}{Revisiting reparameterised gradients}
	Let $Z$ take on values in $\mathbb R^K$ with pdf $q(z|\lambda)$ \\
	
	~ \pause

	The idea is to count on a \emph{standardisation} procedure\\ \pause
	~ a transformation $\mathcal S_\lambda: \mathbb R^K \to \mathbb R^K$ such that \pause
	\begin{equation*}
	\begin{aligned}
	\mathcal S_\lambda(z) &\sim \pi(\epsilon) \\
 	\inv{\mathcal S}_\lambda(\epsilon) &\sim q(z|\lambda)
	\end{aligned}
	\end{equation*} 
	\begin{itemize}
		\item $\pi(\epsilon)$ does not depend on parameters $\lambda$\\
		we call it a \emph{standard} density \pause
		\item $\mathcal S_\lambda(z)$ absorbs dependency on $\lambda$ 
	\end{itemize}

\end{frame}

\begin{frame}{Reparameterised expectations}
	If we are interested in 
	\begin{equation*}
	\begin{aligned}
		&  \E[\alert{q(z|\lambda)}]{g(z)} \pause = \int \alert{q(z|\lambda)} g(z) \textcolor{blue}{\dd z} \\ \pause
		&= \int \underbrace{\alert{\pi(\mathcal S_\lambda(z)) \djac{S_\lambda}{z}}}_{\text{change of density}} g(z) \textcolor{blue}{\dd z} \\ \pause
		&= \int \alert{\pi(\epsilon)} \pause \underbrace{\alert{\djac{\inv{\mathcal S}_\lambda}{\epsilon}^{-1}}}_{\text{inv func theorem}} \pause g(\underbrace{\inv{\mathcal S}_\lambda(\epsilon)}_{z}) \pause \underbrace{\textcolor{blue}{\djac{\inv{\mathcal S}_\lambda}{\epsilon} \dd \epsilon}}_{\text{change of var}} \\ \pause
		&= \int \pi(\epsilon) g(\inv{\mathcal S}_\lambda(\epsilon))\dd \epsilon \pause = \E[\pi(\epsilon)]{g(\inv{\mathcal S}_\lambda(\epsilon)) }
	\end{aligned}
	\end{equation*}
\end{frame}

\begin{frame}{Reparameterised gradients}
	For optimisation, we need tractable gradients
	\begin{equation*}
		\begin{aligned}
			\pdv{\alert{\lambda}}  \E[\alert{q(z|\lambda)}]{g(z)} = \pdv{\alert{\lambda}} \E[\textcolor{blue}{\pi(\epsilon)}]{g(\inv{\mathcal S}_{\alert\lambda}(\epsilon)) }
		\end{aligned}
	\end{equation*} \pause
	since now the measure of integration does not depend on $\lambda$, we can obtain a gradient estimate
	\begin{equation*}
		\begin{aligned}
			&\pdv{\alert{\lambda}}  \E[\alert{q(z|\lambda)}]{g(z)} =  \E[\textcolor{blue}{\pi(\epsilon)}]{\pdv{\alert{\lambda}} g(\inv{\mathcal S}_{\alert\lambda}(\epsilon)) } \\ \pause
			&\overset{\text{MC}}{\approx}  \frac{1}{M} \sum_{\substack{i=1\\ \epsilon_i \sim \pi(\epsilon)}}^M \pdv{\alert{\lambda}} g(\inv{\mathcal S}_{\alert\lambda}(\epsilon_i)) 
		\end{aligned}
	\end{equation*}
\end{frame}

\begin{frame}{Standardisation functions}
	Location-scale family
	\begin{itemize}
		\item a family of distributions where for $F_X(x) = \Prob{X \le x}$ \\
		if $Y=a + b X$, then  $F_Y(y|a, b)=F_X(\frac{z-a}{b})$ \pause
		\item if we can draw from $f_X(x)$, we can draw from $f_Y(y|a,b)$ \pause
		\item the transformation absorbs the parameters $a, b$
		%\item $\frac{z - \mu}{\sigma}$ is the standardisation function \\
		%it's differentiable and invertible\\
		%$z  = \mu + \sigma \epsilon$		
	\end{itemize}
	
	\pause
	
	Examples: Gaussian, Laplace, Cauchy, Uniform
	
\end{frame}

\begin{frame}{Standardisation functions (cont.)}
	Inverse cdf
	\begin{itemize}
		\item for univariate $Z$ with pdf $f_Z(z)$ and cdf $F_Z(z)$
		\begin{equation*}
		\begin{aligned}
			P \sim \mathcal U(0, 1) \qquad Z \sim \inv{F}_Z(P) 
		\end{aligned}		
		\end{equation*}
		where $\inv{F}_Z(p)$ is the \emph{quantile function}
	\end{itemize}
	
	~ \pause
	
	Gumbel distribution
	\begin{itemize}
		\item $f_Z(z|\mu, \beta) = \beta^{-1}\exp(-z -\exp(-z))$ 
		\item $F_Z(z|\mu, \beta) = \exp(-\exp(-\frac{z-\mu}{\beta}))$
		\item $\inv{F}_Z(p) = \mu - \beta \log( - \log p)$
	\end{itemize}
\end{frame}

\begin{frame}{Beyond}

	Many interesting densities are not location-scale families
	\begin{itemize}
		\item e.g. Beta, Gamma
	\end{itemize} \pause
	
	The inverse cdf of a multivariate rv is seldom known in closed-form
	\begin{itemize}
		\item Dirichlet, von Mises-Fisher
	\end{itemize}

\end{frame}

\section{ADVI}

\begin{frame}{Automatic Differentiation VI}
	Motivation
	\begin{itemize}
		\item many models have intractable posteriors\\
		their normalising constants (evidence) lacks analytic solutions \pause
		\item but many models are differentiable\\
		that's the main constraint for using NNs \pause
	\end{itemize}

	Reparameterised gradients are a step towards automatising VI for differentiable models \pause
	\begin{itemize}
		\item but not every model of interest employs rvs for which a standardisation function is known
	\end{itemize}
	
\end{frame}

\begin{frame}{Example}
	Suppose we have some ordinal data which we assume to be Poisson-distributed
	\begin{equation*}
		X|\lambda \sim \Poisson(\lambda)
	\end{equation*}
	and suppose we want to impose 
\end{frame}

\begin{frame}{Differentiable models}

	We focus on \emph{differentiable probability models}
	\begin{equation*}
		p(x,z) = p(x|z)p(z)
	\end{equation*}
	\pause
	\begin{itemize}
		\item members of this class have continuous latent variables $z$\\ \pause
		\item and the gradient $\grad_z \log p(x,z)$ is valid within the \emph{support} of the prior 
		$\supp(p(z)) = \{ z \in \mathbb R^K : p(z) > 0 \} \subseteq \mathbb R^K$
	\end{itemize}
	
\end{frame}

\begin{frame}{VI optimisation problem}
	Let's focus on the design and optimisation of the variational approximation
	\begin{equation*}
		\argmin_{\balert{q(z)}} \KL{\balert{q(z)}}{\alert{p(z|x)}}
	\end{equation*}

	\pause
	
	To automatise the search for a variational approximation $\balert{q(z)}$ we must ensure that\\
	 \begin{equation*}
	 	\supp(\balert{q(z)}) \subseteq \supp(\alert{p(z|x)})
	\end{equation*} \pause
	
	\vspace{-10pt}
	 \begin{itemize}
	 	\item otherwise KL is not defined\\
		$\KL{q}{p} = \E[q]{\log q} - \E[q]{\log p} = \infty$
	\end{itemize}	
	 
\end{frame}

\begin{frame}{Support matching constraint}

	So let's constrain $q(z)$ to a family $\mathcal Q$ whose support is included in the support of the \alert{posterior}
	 \begin{equation*}
		\argmin_{\balert{q(z)} \in \mathcal Q} \KL{\balert{q(z)}}{\alert{p(z|x)}}
	\end{equation*}
	where
	\begin{equation*}
	 	\mathcal Q = \{\balert{q(z)}: \supp(\balert{q(z)}) \subseteq \supp(\alert{p(z|x)}) \}
	\end{equation*}
	
	\pause
	
	\alert{But what is the support of $\alert{p(z|x)}$?} \pause
	 \begin{itemize}
		\item typically the same as the support of $\galert{p(z)}$\\ \pause
		as long as $p(x,z) > 0$ if $p(z) > 0$		
	 \end{itemize}
	 
\end{frame}

\begin{frame}{Parametric family}

	So let's constrain $q(z)$ to a family $\mathcal Q$ whose support is included in the support of the \galert{prior}
	 \begin{equation*}
		\argmin_{\balert{q(z)} \in \mathcal Q} \KL{\balert{q(z)}}{\alert{p(z|x)}}
	\end{equation*}
	where
	\begin{equation*}
	 	\mathcal Q = \{\balert{q(z; \phi)}: \phi \in \Phi, \supp(\balert{q(z; \phi)}) \subseteq \supp(\galert{p(z)})  \}
	\end{equation*}
	\vspace{-10pt} \pause
	 \begin{itemize}
	 	\item a parameter vector $\phi$ picks out a member of the family
	\end{itemize}

\end{frame}

\begin{frame}{Constrained optimisation for the ELBO}

	We maximise the ELBO 
	\begin{equation*}
		\argmax_{\phi \in \Phi} \E[\balert{q(z; \phi)}]{\log p(x, z)} + \Ent{\balert{q(z|\phi)}}
	\end{equation*}
	subject to
	 \begin{equation*}
		\mathcal Q = \{\balert{q(z; \phi)}: \phi \in \Phi, \supp(\balert{q(z; \phi)}) \subseteq \supp(\galert{p(z)})  \}
	\end{equation*}
	\pause
	
	\vspace{-10pt}
	There are really two constraints here\pause
	\begin{itemize}
		\item support matching constraint \pause
		\item $\Phi$ can be an intricate subset of $\mathbb R^D$\\ \pause
		e.g. univariate Gaussian location lives in $\mathbb R$ but scale lives in $\mathbb R_{>0}$
	\end{itemize}

\end{frame}

\begin{frame}{ADVI}
	
	
	
	From the point of view of a black-box procedure, this objective poses two problems
	\begin{enumerate}
		\item intractable expectations \pause Reparameterised Gradients!
		\item custom $\supp(q(z; \phi))$ 
	\end{enumerate}
	
	Idea
	\begin{enumerate}
		\item let's find a way to transform $\supp(p(z))$ to the complete real coordinate space
		\item then we pick a variational family over the complete real coordinate space for which a standardisation exists!
	\end{enumerate}
	
\end{frame}


\begin{frame}[allowframebreaks]
\bibliographystyle{plainnat}
\bibliography{../../VI}
\end{frame}


\end{document}